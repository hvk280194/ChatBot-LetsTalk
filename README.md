# ğŸ’¬ Let's Learn

**Letâ€™s Learn** is a modular **AI chat application** built using **FastAPI** for backend APIs and **Streamlit** for the frontend interface.
It integrates **Hugging Face models** (via the Inference API) or **Ollama** for local inference and supports **asynchronous streaming** for smooth, real-time chat responses.

---

## ğŸ–¼ï¸ Preview


<p align="center">
  <img src="assets/demo.png" width="600">
</p>

---

## âš™ï¸ Tech Stack

| Layer              | Technology                |
| :----------------- | :------------------------ |
| **Frontend**       | Streamlit                 |
| **Backend**        | FastAPI + Uvicorn         |
| **LLM Providers**  | Hugging Face Hub / Ollama |
| **Async Engine**   | Python `asyncio` + SSE    |  |

---

## ğŸš€ Quickstart

### 1ï¸âƒ£ Setup environment

```bash
python -m venv .venv
source .venv/bin/activate      # On Windows: .venv\Scripts\activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure environment

Open `.env` and **add your Hugging Face API token**:

```bash
HFHUB_API_TOKEN=hf_your_token_here
```

ğŸ‘‰ If you donâ€™t have one yet:

1. Go to [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
2. Click **â€œNew Tokenâ€** â†’ choose type **Read**
3. Copy it and paste into your `.env` file as shown above.

---

### 4ï¸âƒ£ Start backend (FastAPI)

```bash
uvicorn app.main:app --reload --port 8000
```

### 5ï¸âƒ£ Start frontend (Streamlit)

In a new terminal:

```bash
streamlit run frontend/app.py --server.port 8501
```

âœ… Open the Streamlit app at [http://localhost:8501](http://localhost:8501)

---

## ğŸ§© Features

* âš¡ **FastAPI backend** for scalable, async inference
* ğŸ’¬ **Streamlit chat interface** with real-time updates
* ğŸ”Œ **Multiple model providers** (Hugging Face, Ollama)
* ğŸ§  **Session memory** for chat continuity
* ğŸ§° Modular design for easy extensions (tools, agents, RAG, etc.)

---

## âš™ï¸ Environment Variables (`.env`)

```bash
# Default: Hugging Face setup
PROVIDER=hfhub
HFHUB_REPO_ID=mistralai/Mistral-7B-Instruct-v0.2
HFHUB_API_TOKEN=hf_your_token_here
SSE_CHUNK_DELAY_MS=0

# Optional: Ollama setup
PROVIDER=ollama
OLLAMA_MODEL=mistral
OLLAMA_BASE_URL=http://localhost:11434
```

